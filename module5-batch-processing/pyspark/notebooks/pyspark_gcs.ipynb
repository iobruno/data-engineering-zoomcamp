{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "232d3f5c-f2dd-481f-ae8e-ef094e730a8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from os import environ as env\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql.functions import col, dayofmonth\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099a62ca-14ce-448e-bb21-b4a8c4bb173d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Spark Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed70919a-1c75-4fef-96b2-e9a883189a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -nc https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-latest-hadoop2.jar -P /tmp/spark_jars -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69f21eac-4d77-4023-8b27-089928c16c23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conf = (\n",
    "    SparkConf()\n",
    "        .setMaster('local[*]')\n",
    "        .setAppName(\"pyspark-playground\")\n",
    "        .set(\"spark.cores.max\", 4)\n",
    "        .set(\"spark.driver.memory\", \"2g\")\n",
    "        .set(\"spark.executor.memory\", \"8g\")\n",
    "        .set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "        .set(\"spark.jars\", \"/tmp/spark_jars/gcs-connector-latest-hadoop2.jar\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc346ebf-2848-42f6-9613-cf810673ce14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/28 05:41:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "hadoop_conf = sc._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.AbstractFileSystem.gs.impl\",  \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "hadoop_conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "hadoop_conf.set(\"fs.gs.auth.service.account.enable\", \"true\")\n",
    "hadoop_conf.set(\"fs.gs.auth.service.account.json.keyfile\", env[\"GOOGLE_APPLICATION_CREDENTIALS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a1cf602-ea4f-4525-9394-7fd462681a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cfc512-824a-4c99-af3c-66f384aba4f9",
   "metadata": {},
   "source": [
    "## Load Datasets from GCS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfc13a3-43d9-491d-abf5-fd00e3d55214",
   "metadata": {
    "tags": []
   },
   "source": [
    "### FHV Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22456092-e53c-4df1-baf7-f78861433a32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "raw_fhv = spark.read.parquet(\"gs://iobruno-lakehouse-raw/nyc_tlc_dataset/fhv_trip_data/fhv_tripdata_2019-10.snappy.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "475dad1e-6770-4987-81c3-0f0002e07174",
   "metadata": {},
   "outputs": [],
   "source": [
    "fhv = raw_fhv.select(\n",
    "    col('dispatching_base_num'),\n",
    "    col('Affiliated_base_number').alias('affiliated_base_num'),\n",
    "    col('PUlocationID').alias('pickup_location_id'),\n",
    "    col('DOlocationID').alias('dropoff_location_id'),\n",
    "    col('pickup_datetime').cast('timestamp'),\n",
    "    col('dropOff_datetime').cast('timestamp'),\n",
    "    col('SR_Flag').alias('sr_flag'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbbe6e36-94a1-48fb-875a-9d9ebd5dc13b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fhv.createOrReplaceTempView('fhv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b48b25d-8567-4931-b853-dc00ccb8dc0d",
   "metadata": {},
   "source": [
    "### Zone Lookup Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84329e08-b7f0-4b41-9b85-0cf08ef8c13d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "zones_schema = StructType([\n",
    "    StructField(\"LocationID\", IntegerType(), True),\n",
    "    StructField(\"Borough\", StringType(), True),\n",
    "    StructField(\"Zone\", StringType(), True),\n",
    "    StructField(\"service_zone\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bda76359-88cc-4b32-a7c7-ff3b48c51097",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "zones = spark.read.csv(\n",
    "    path=\"gs://iobruno-lakehouse-raw/nyc_tlc_dataset/zone_lookup/*.csv.gz\",\n",
    "    header=True,\n",
    "    inferSchema=False,\n",
    "    schema=zones_schema,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb0be05b-f47d-4044-87f1-c8227bcef1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "|         6|Staten Island|Arrochar/Fort Wad...|   Boro Zone|\n",
      "|         7|       Queens|             Astoria|   Boro Zone|\n",
      "|         8|       Queens|        Astoria Park|   Boro Zone|\n",
      "|         9|       Queens|          Auburndale|   Boro Zone|\n",
      "|        10|       Queens|        Baisley Park|   Boro Zone|\n",
      "|        11|     Brooklyn|          Bath Beach|   Boro Zone|\n",
      "|        12|    Manhattan|        Battery Park| Yellow Zone|\n",
      "|        13|    Manhattan|   Battery Park City| Yellow Zone|\n",
      "|        14|     Brooklyn|           Bay Ridge|   Boro Zone|\n",
      "|        15|       Queens|Bay Terrace/Fort ...|   Boro Zone|\n",
      "|        16|       Queens|             Bayside|   Boro Zone|\n",
      "|        17|     Brooklyn|             Bedford|   Boro Zone|\n",
      "|        18|        Bronx|        Bedford Park|   Boro Zone|\n",
      "|        19|       Queens|           Bellerose|   Boro Zone|\n",
      "|        20|        Bronx|             Belmont|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zones.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "139a31ea-2546-4b47-b728-172d5df9b587",
   "metadata": {},
   "outputs": [],
   "source": [
    "zones = zones.select(\n",
    "    col('LocationID').alias('location_id'),\n",
    "    col('Borough').alias('borough'),\n",
    "    col('Zone').alias('zone'),\n",
    "    col('service_zone')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7347b30-97d5-4b47-99d6-63d38a579ed2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "zones.createOrReplaceTempView('zones')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ca37d5-a578-4572-9713-f11880cf8239",
   "metadata": {},
   "source": [
    "## Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3835f303-8b53-4803-ba43-adb6873af658",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Question 1\n",
    "\n",
    "**Install Spark and PySpark** \n",
    "\n",
    "- Install Spark\n",
    "- Run PySpark\n",
    "- Create a local spark session\n",
    "- Execute spark.version.\n",
    "\n",
    "What's the output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f764da54-67b5-4d66-acac-f34bccc5a0de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.1'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d835f4-cba9-40a9-b499-930ebb7585f9",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "**FHV October 2019**  \n",
    "- Read the October 2019 FHV into a Spark Dataframe with a schema as we did in the lessons\n",
    "- Repartition the Dataframe to 6 partitions and save it to parquet.\n",
    "\n",
    "What is the average size of the Parquet (ending with .parquet extension) Files that were created (in MB)?  \n",
    "Select the answer which most closely matches\n",
    "- [ ] 1 MB\n",
    "- [x] 6 MB\n",
    "- [ ] 25 MB\n",
    "- [ ] 87 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64f6fd1c-d391-4915-835e-f31fad85a88e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = spark.sql(\"\"\"SELECT * FROM fhv\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16b75697-43b7-40fd-8425-d1d660d8a4af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.repartition(6)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .parquet(\"/tmp/dtc-homework/fhv/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49ddd303-5458-409c-8cdc-5b20076e56a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 76768\n",
      "-rw-r--r--@ 1 iobruno  wheel     0B Aug 28 05:41 _SUCCESS\n",
      "-rw-r--r--@ 1 iobruno  wheel   6.3M Aug 28 05:41 part-00000-4cfe1cd5-1982-42d1-b073-b9cebc35a241-c000.snappy.parquet\n",
      "-rw-r--r--@ 1 iobruno  wheel   6.2M Aug 28 05:41 part-00001-4cfe1cd5-1982-42d1-b073-b9cebc35a241-c000.snappy.parquet\n",
      "-rw-r--r--@ 1 iobruno  wheel   6.2M Aug 28 05:41 part-00002-4cfe1cd5-1982-42d1-b073-b9cebc35a241-c000.snappy.parquet\n",
      "-rw-r--r--@ 1 iobruno  wheel   6.2M Aug 28 05:41 part-00003-4cfe1cd5-1982-42d1-b073-b9cebc35a241-c000.snappy.parquet\n",
      "-rw-r--r--@ 1 iobruno  wheel   6.2M Aug 28 05:41 part-00004-4cfe1cd5-1982-42d1-b073-b9cebc35a241-c000.snappy.parquet\n",
      "-rw-r--r--@ 1 iobruno  wheel   6.2M Aug 28 05:41 part-00005-4cfe1cd5-1982-42d1-b073-b9cebc35a241-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls -lh /tmp/dtc-homework/fhv/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545a31ba-76c8-4964-a4c1-ba1279eea57c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Question 3\n",
    "\n",
    "**Count records**  \n",
    "\n",
    "How many taxi trips were there on the 15th of October?  \n",
    "\n",
    "Consider only trips that started on the 15th of October\n",
    "- [ ] 108,164\n",
    "- [ ] 12,856\n",
    "- [ ] 452,470\n",
    "- [x] 62,610"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9d6d612-4f82-4a2e-af90-9ada87b4d359",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(month=10, day=15, num_trips=62610)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    with trips_per_month_day as (\n",
    "        select\n",
    "            month(pickup_datetime) as month,\n",
    "            dayofmonth(pickup_datetime) as day, \n",
    "            count(1) as num_trips\n",
    "        from\n",
    "            fhv\n",
    "        group by \n",
    "            month(pickup_datetime), \n",
    "            dayofmonth(pickup_datetime)\n",
    "    )\n",
    "\n",
    "    select * \n",
    "    from trips_per_month_day \n",
    "    where \n",
    "        month = 10 \n",
    "        and day = 15\n",
    "\"\"\").take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7a4640a-d3c9-4af2-a402-eb4ec9322cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(month=10, day=15, num_trips=62610)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "        select\n",
    "            month(pickup_datetime) as month,\n",
    "            dayofmonth(pickup_datetime) as day, \n",
    "            count(1) as num_trips\n",
    "        from\n",
    "            fhv\n",
    "        group by \n",
    "            month(pickup_datetime), \n",
    "            dayofmonth(pickup_datetime)\n",
    "        having \n",
    "            month = 10 and \n",
    "            day = 15\n",
    "\"\"\").take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0e0205-73fb-4462-b280-b71bb74cc680",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Question 4: \n",
    "\n",
    "**Longest trip for each day**  \n",
    "\n",
    "What is the length of the longest trip in the dataset in hours?\n",
    "- [x] 631,152.50 Hours\n",
    "- [ ] 243.44 Hours\n",
    "- [ ] 7.68 Hours\n",
    "- [ ] 3.32 Hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0792b43b-36a6-463a-ae42-02c316d5cc26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(duration_in_hours=631152.5, rnk=1)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    with trip_records AS (\n",
    "        select\n",
    "            pickup_location_id,\n",
    "            dropoff_location_id,\n",
    "            pickup_datetime,\n",
    "            dropoff_datetime,\n",
    "            (cast(dropoff_datetime as long) - cast(pickup_datetime as long))/3600 as duration_in_hours\n",
    "        from \n",
    "            fhv\n",
    "    )\n",
    "\n",
    "    select \n",
    "        duration_in_hours,\n",
    "        dense_rank() over (order by duration_in_hours desc) as rnk\n",
    "    from\n",
    "        trip_records t\n",
    "\n",
    "\"\"\").take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "151d8a17-ee68-4015-82a8-19132da81a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(duration_in_hours=631152.5, rnk=1)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    with trip_records AS (\n",
    "        select\n",
    "            pickup_location_id,\n",
    "            dropoff_location_id,\n",
    "            pickup_datetime,\n",
    "            dropoff_datetime,\n",
    "            (cast(dropoff_datetime as long) - cast(pickup_datetime as long))/3600 as duration_in_hours\n",
    "        from \n",
    "            fhv\n",
    "    )\n",
    "\n",
    "    select \n",
    "        duration_in_hours,\n",
    "        dense_rank() over (order by duration_in_hours desc) as rnk\n",
    "    from\n",
    "        trip_records t\n",
    "\"\"\").take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bf8dca-ecd7-484b-8006-2d6eccc8162b",
   "metadata": {},
   "source": [
    "### Question 5: \n",
    "\n",
    "**User Interface**\n",
    "\n",
    "Sparkâ€™s User Interface which shows the application's dashboard runs on which local port?\n",
    "\n",
    "- [ ] 80\n",
    "- [ ] 443\n",
    "- [x] 4040\n",
    "- [ ] 8080"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e08bed-582a-4aa6-8537-9ddad4b513c8",
   "metadata": {},
   "source": [
    "### Question 6: \n",
    "\n",
    "**Least frequent pickup location zone**\n",
    "\n",
    "Load the zone lookup data into a temp view in Spark [Zone Data](https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv)  \n",
    "\n",
    "Using the zone lookup data and the FHV October 2019 data, what is the name of the LEAST frequent pickup location Zone?\n",
    "- [ ] East Chelsea\n",
    "- [x] Jamaica Bay\n",
    "- [ ] Union Sq\n",
    "- [ ] Crown Heights North"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15ab7cf3-48f4-471b-9164-6568899c3ce6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(zone='Jamaica Bay', num_trips=1, rnk=1)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    with trips_per_location AS (\n",
    "        select\n",
    "            pickup_location_id,\n",
    "            count(1) as num_trips,\n",
    "            dense_rank() over (order by count(1) asc) as rnk\n",
    "        from\n",
    "            fhv\n",
    "        group by\n",
    "            pickup_location_id\n",
    "    )\n",
    "\n",
    "    select\n",
    "        pu.zone,\n",
    "        t.num_trips,\n",
    "        t.rnk\n",
    "    from\n",
    "        trips_per_location t\n",
    "    inner join\n",
    "        zones pu on t.pickup_location_id = pu.location_id\n",
    "    where\n",
    "        rnk = 1\n",
    "\"\"\").take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5e6051-1fb5-45bd-8241-a127d4fcae20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
